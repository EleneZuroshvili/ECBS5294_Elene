{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2332f652",
   "metadata": {},
   "source": [
    "# Day 3, Block A: Data Pipelines & Real-World Validation\n",
    "\n",
    "**Duration:** 100 minutes (13:30‚Äì15:10)\n",
    "**Course:** ECBS5294 - Introduction to Data Science: Working with Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. Explain the **bronze ‚Üí silver ‚Üí gold** pipeline pattern and why it matters\n",
    "2. Design idempotent data transformations\n",
    "3. Write **assertions** to validate data quality programmatically\n",
    "4. Identify and handle common real-world data problems (dates, types, nulls)\n",
    "5. Apply the **pipeline pattern** to a real dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f29c73",
   "metadata": {},
   "source": [
    "## 1. Why Data Pipelines?\n",
    "\n",
    "### The Problem: One-Off Analysis Doesn't Scale\n",
    "\n",
    "**You've hit the wall when:**\n",
    "- Data updates regularly\n",
    "- Multiple people need consistent results\n",
    "- Stakeholders ask \"how did you get this number?\"\n",
    "- Requirements change\n",
    "\n",
    "**The solution:** A systematic, repeatable pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd9118",
   "metadata": {},
   "source": [
    "## 2. The Bronze-Silver-Gold Pattern\n",
    "\n",
    "> **\"Preserve the original, clean incrementally, aggregate deliberately.\"**\n",
    "\n",
    "#### **Bronze Layer: Raw Ingestion**\n",
    "- Preserve original data exactly as received\n",
    "- No transformations\n",
    "- Keep everything‚Äîeven if it looks wrong\n",
    "\n",
    "#### **Silver Layer: Clean & Validated**\n",
    "- Analysis-ready data\n",
    "- Fix types, handle nulls, validate\n",
    "- Document what was fixed\n",
    "\n",
    "#### **Gold Layer: Business Metrics**\n",
    "- Aggregated, joined, ready for reporting\n",
    "- Pre-computed KPIs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "con = duckdb.connect(':memory:')\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cb304",
   "metadata": {},
   "source": [
    "## 3. Scenario 1: E-Commerce Pipeline\n",
    "\n",
    "**Business Context:** You're an analyst at a Brazilian e-commerce company. Orders and customer data arrive daily. You need reliable, repeatable metrics.\n",
    "\n",
    "**Data:** 1,000 orders from Olist (familiar from Day 2)\n",
    "\n",
    "**Pipeline Goal:** Bronze ‚Üí Silver ‚Üí Gold with validations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373f482",
   "metadata": {},
   "source": [
    "### Bronze Layer: Raw Ingestion\n",
    "\n",
    "**Goal:** Load data exactly as received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c7f6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRONZE: Load raw data\n",
    "print(\"=== BRONZE LAYER ===\\n\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE bronze_orders AS\n",
    "    SELECT * FROM '../../data/day3/teaching/olist_orders_subset.csv'\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE bronze_customers AS\n",
    "    SELECT * FROM '../../data/day3/teaching/olist_customers_subset.csv'\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE bronze_order_items AS\n",
    "    SELECT * FROM '../../data/day3/teaching/olist_order_items_subset.csv'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Loaded {con.execute('SELECT COUNT(*) FROM bronze_orders').fetchone()[0]} orders\")\n",
    "print(\"‚úÖ Bronze layer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51983f",
   "metadata": {},
   "source": [
    "### Silver Layer: Clean & Validate\n",
    "\n",
    "**Goal:** Transform into analysis-ready format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3db3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILVER: Clean and validate\n",
    "print(\"=== SILVER LAYER ===\\n\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE silver_orders AS\n",
    "    SELECT\n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_status,\n",
    "        TRY_CAST(order_purchase_timestamp AS TIMESTAMP) as order_date\n",
    "    FROM bronze_orders\n",
    "    WHERE order_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE silver_order_items AS\n",
    "    SELECT\n",
    "        order_id,\n",
    "        product_id,\n",
    "        CAST(price AS DOUBLE) as price,\n",
    "        CAST(freight_value AS DOUBLE) as freight\n",
    "    FROM bronze_order_items\n",
    "    WHERE order_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Created {con.execute('SELECT COUNT(*) FROM silver_orders').fetchone()[0]} clean orders\")\n",
    "print(\"‚úÖ Silver layer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64811321",
   "metadata": {},
   "source": [
    "### Validation: Prove Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c55a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "print(\"=== VALIDATION ===\\n\")\n",
    "\n",
    "# Check 1: Primary key uniqueness\n",
    "order_count = con.execute(\"SELECT COUNT(*) FROM silver_orders\").fetchone()[0]\n",
    "order_unique = con.execute(\"SELECT COUNT(DISTINCT order_id) FROM silver_orders\").fetchone()[0]\n",
    "\n",
    "print(f\"‚úì Order IDs unique? {order_count == order_unique}\")\n",
    "assert order_count == order_unique, \"Duplicate order IDs!\"\n",
    "\n",
    "# Check 2: No null critical fields\n",
    "null_ids = con.execute(\"SELECT COUNT(*) FROM silver_orders WHERE order_id IS NULL\").fetchone()[0]\n",
    "print(f\"‚úì No NULL order IDs? {null_ids == 0}\")\n",
    "assert null_ids == 0, \"NULL order IDs found!\"\n",
    "\n",
    "# Check 3: Foreign key integrity\n",
    "orphans = con.execute(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM silver_order_items i\n",
    "    LEFT JOIN silver_orders o ON i.order_id = o.order_id\n",
    "    WHERE o.order_id IS NULL\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"‚úì All items have valid orders? {orphans == 0}\")\n",
    "assert orphans == 0, \"Orphaned items found!\"\n",
    "\n",
    "print(\"\\n‚úÖ ALL VALIDATIONS PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3225daa",
   "metadata": {},
   "source": [
    "### Gold Layer: Business Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD: Business metrics\n",
    "print(\"=== GOLD LAYER ===\\n\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE gold_daily_sales AS\n",
    "    SELECT\n",
    "        CAST(o.order_date AS DATE) as date,\n",
    "        COUNT(DISTINCT o.order_id) as num_orders,\n",
    "        SUM(i.price + i.freight) as total_revenue\n",
    "    FROM silver_orders o\n",
    "    INNER JOIN silver_order_items i ON o.order_id = i.order_id\n",
    "    WHERE o.order_date IS NOT NULL\n",
    "    GROUP BY CAST(o.order_date AS DATE)\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "result = con.execute(\"SELECT * FROM gold_daily_sales LIMIT 5\").df()\n",
    "print(\"Daily sales summary:\")\n",
    "display(result)\n",
    "print(\"\\n‚úÖ Gold layer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00950cf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Scenario 2: Messy Retail Data Pipeline\n",
    "\n",
    "**Business Context:** You've inherited a cafe's sales data export. It's full of data quality issues‚Äîthe reality of working with real-world data.\n",
    "\n",
    "**Data:** 10,000 cafe transactions (from Day 1's messy dataset)\n",
    "\n",
    "**Pipeline Goal:** Handle multiple NULL representations, validate calculations, detect outliers\n",
    "\n",
    "**Key Difference from Scenario 1:** This focuses on **data quality** issues, not relational integrity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88873b37",
   "metadata": {},
   "source": [
    "### Bronze Layer: Load Messy Data\n",
    "\n",
    "First, let's see what we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32631dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T10:53:34.737313Z",
     "iopub.status.busy": "2025-10-22T10:53:34.737255Z",
     "iopub.status.idle": "2025-10-22T10:53:34.767858Z",
     "shell.execute_reply": "2025-10-22T10:53:34.767653Z"
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE: Load raw cafe data\n",
    "print(\"=== BRONZE LAYER (Cafe Sales) ===\\n\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE bronze_cafe_sales AS\n",
    "    SELECT * FROM '../../data/day1/dirty_cafe_sales.csv'\n",
    "\"\"\")\n",
    "\n",
    "# Check what we loaded\n",
    "print(\"Sample of raw data:\")\n",
    "display(con.execute(\"SELECT * FROM bronze_cafe_sales LIMIT 5\").df())\n",
    "\n",
    "# Check for data quality issues\n",
    "print(\"\\nData quality snapshot:\")\n",
    "print(f\"Total records: {con.execute('SELECT COUNT(*) FROM bronze_cafe_sales').fetchone()[0]}\")\n",
    "\n",
    "# Show NULL counts\n",
    "null_summary = con.execute(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) - COUNT(\"Transaction ID\") as null_txn_id,\n",
    "        COUNT(*) - COUNT(\"Item\") as null_item,\n",
    "        COUNT(*) - COUNT(\"Total Spent\") as null_total,\n",
    "        COUNT(*) - COUNT(\"Payment Method\") as null_payment,\n",
    "        COUNT(*) - COUNT(\"Transaction Date\") as null_date\n",
    "    FROM bronze_cafe_sales\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nNULL counts:\")\n",
    "display(null_summary)\n",
    "\n",
    "# Show sentinel values\n",
    "print(\"\\nSentinel value counts:\")\n",
    "display(con.execute(\"\"\"\n",
    "    SELECT\n",
    "        SUM(CASE WHEN \"Item\" = 'ERROR' OR \"Item\" = 'UNKNOWN' THEN 1 ELSE 0 END) as error_unknown_items,\n",
    "        SUM(CASE WHEN \"Total Spent\" = 'ERROR' THEN 1 ELSE 0 END) as error_totals,\n",
    "        SUM(CASE WHEN \"Transaction Date\" = 'ERROR' THEN 1 ELSE 0 END) as error_dates\n",
    "    FROM bronze_cafe_sales\n",
    "\"\"\").df())\n",
    "\n",
    "print(\"\\n‚úÖ Bronze layer complete (messy data preserved)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ae3f5",
   "metadata": {},
   "source": [
    "### Silver Layer: Clean & Validate\n",
    "\n",
    "**Cleaning strategy:**\n",
    "1. Replace sentinel values (\"ERROR\", \"UNKNOWN\") with NULL\n",
    "2. Fix types (convert strings to numbers/dates)\n",
    "3. Validate business rules (quantity > 0, price ‚â• 0)\n",
    "4. Check calculation: `Total Spent = Quantity √ó Price Per Unit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa61bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T10:53:34.768899Z",
     "iopub.status.busy": "2025-10-22T10:53:34.768821Z",
     "iopub.status.idle": "2025-10-22T10:53:34.776435Z",
     "shell.execute_reply": "2025-10-22T10:53:34.776125Z"
    }
   },
   "outputs": [],
   "source": [
    "# SILVER: Clean the data\n",
    "print(\"=== SILVER LAYER (Cafe Sales) ===\\n\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE silver_cafe_sales AS\n",
    "    SELECT\n",
    "        \"Transaction ID\" as transaction_id,\n",
    "        -- Replace sentinel values with NULL\n",
    "        CASE \n",
    "            WHEN \"Item\" IN ('ERROR', 'UNKNOWN', '') THEN NULL \n",
    "            ELSE \"Item\" \n",
    "        END as item,\n",
    "        -- Convert to integers, handle NULLs\n",
    "        TRY_CAST(\"Quantity\" AS INTEGER) as quantity,\n",
    "        TRY_CAST(\"Price Per Unit\" AS DOUBLE) as price_per_unit,\n",
    "        -- Handle \"ERROR\" in Total Spent\n",
    "        TRY_CAST(\n",
    "            CASE WHEN \"Total Spent\" = 'ERROR' THEN NULL ELSE \"Total Spent\" END\n",
    "            AS DOUBLE\n",
    "        ) as total_spent,\n",
    "        -- Clean payment method\n",
    "        CASE \n",
    "            WHEN \"Payment Method\" IN ('ERROR', 'UNKNOWN', '') THEN NULL \n",
    "            ELSE \"Payment Method\" \n",
    "        END as payment_method,\n",
    "        -- Clean location\n",
    "        CASE \n",
    "            WHEN \"Location\" IN ('ERROR', 'UNKNOWN', '') THEN NULL \n",
    "            ELSE \"Location\" \n",
    "        END as location,\n",
    "        -- Parse dates\n",
    "        TRY_CAST(\n",
    "            CASE WHEN \"Transaction Date\" = 'ERROR' THEN NULL ELSE \"Transaction Date\" END\n",
    "            AS DATE\n",
    "        ) as transaction_date\n",
    "    FROM bronze_cafe_sales\n",
    "    WHERE \"Transaction ID\" IS NOT NULL  -- Keep only records with valid IDs\n",
    "\"\"\")\n",
    "\n",
    "# Check results\n",
    "cleaned_count = con.execute(\"SELECT COUNT(*) FROM silver_cafe_sales\").fetchone()[0]\n",
    "print(f\"Cleaned records: {cleaned_count}\")\n",
    "\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(con.execute(\"SELECT * FROM silver_cafe_sales LIMIT 5\").df())\n",
    "\n",
    "print(\"\\n‚úÖ Silver layer complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850c9fb",
   "metadata": {},
   "source": [
    "### Validation: Business Rules & Data Quality\n",
    "\n",
    "**Different validations than Scenario 1:**\n",
    "- Value range checks (no negative prices/quantities)\n",
    "- Categorical domain validation (only known items/payment methods)\n",
    "- Calculation validation (does total = quantity √ó price?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b1800",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T10:53:34.777455Z",
     "iopub.status.busy": "2025-10-22T10:53:34.777401Z",
     "iopub.status.idle": "2025-10-22T10:53:34.781911Z",
     "shell.execute_reply": "2025-10-22T10:53:34.781724Z"
    }
   },
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "print(\"=== VALIDATION (Cafe Sales) ===\\n\")\n",
    "\n",
    "# Check 1: Primary key uniqueness\n",
    "txn_count = con.execute(\"SELECT COUNT(*) FROM silver_cafe_sales\").fetchone()[0]\n",
    "txn_unique = con.execute(\"SELECT COUNT(DISTINCT transaction_id) FROM silver_cafe_sales\").fetchone()[0]\n",
    "print(f\"‚úì Transaction IDs unique? {txn_count == txn_unique}\")\n",
    "assert txn_count == txn_unique, \"Duplicate transaction IDs!\"\n",
    "\n",
    "# Check 2: Value ranges (for non-NULL records)\n",
    "negative_qty = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM silver_cafe_sales \n",
    "    WHERE quantity IS NOT NULL AND quantity < 0\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"‚úì No negative quantities? {negative_qty == 0}\")\n",
    "assert negative_qty == 0, f\"Found {negative_qty} negative quantities!\"\n",
    "\n",
    "negative_price = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) \n",
    "    FROM silver_cafe_sales \n",
    "    WHERE price_per_unit IS NOT NULL AND price_per_unit < 0\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"‚úì No negative prices? {negative_price == 0}\")\n",
    "assert negative_price == 0, f\"Found {negative_price} negative prices!\"\n",
    "\n",
    "# Check 3: Categorical domain validation\n",
    "valid_items = ['Coffee', 'Sandwich', 'Cake', 'Cookie', 'Salad', 'Smoothie', 'Juice']\n",
    "invalid_items = con.execute(f\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM silver_cafe_sales\n",
    "    WHERE item IS NOT NULL AND item NOT IN {tuple(valid_items)}\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"‚úì All items in valid domain? {invalid_items == 0}\")\n",
    "# Note: We don't assert here - unknown items might be valid in real business!\n",
    "if invalid_items > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Warning: {invalid_items} records with unexpected items (might be new products)\")\n",
    "\n",
    "# Check 4: Calculation validation (within 1 cent tolerance for floating point)\n",
    "calc_errors = con.execute(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM silver_cafe_sales\n",
    "    WHERE \n",
    "        quantity IS NOT NULL \n",
    "        AND price_per_unit IS NOT NULL \n",
    "        AND total_spent IS NOT NULL\n",
    "        AND ABS(total_spent - (quantity * price_per_unit)) > 0.01\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"‚úì Total = Quantity √ó Price? {calc_errors == 0} calculation errors\")\n",
    "if calc_errors > 0:\n",
    "    print(f\"  ‚ö†Ô∏è  Warning: {calc_errors} records with calculation mismatches\")\n",
    "    # Show example\n",
    "    print(\"\\n  Example mismatch:\")\n",
    "    display(con.execute(\"\"\"\n",
    "        SELECT transaction_id, quantity, price_per_unit, total_spent,\n",
    "               (quantity * price_per_unit) as calculated_total,\n",
    "               ABS(total_spent - (quantity * price_per_unit)) as difference\n",
    "        FROM silver_cafe_sales\n",
    "        WHERE \n",
    "            quantity IS NOT NULL \n",
    "            AND price_per_unit IS NOT NULL \n",
    "            AND total_spent IS NOT NULL\n",
    "            AND ABS(total_spent - (quantity * price_per_unit)) > 0.01\n",
    "        LIMIT 3\n",
    "    \"\"\").df())\n",
    "\n",
    "# Check 5: Date range validation\n",
    "date_range = con.execute(\"\"\"\n",
    "    SELECT MIN(transaction_date) as earliest, MAX(transaction_date) as latest\n",
    "    FROM silver_cafe_sales\n",
    "    WHERE transaction_date IS NOT NULL\n",
    "\"\"\").df()\n",
    "print(f\"\\n‚úì Date range: {date_range['earliest'].iloc[0]} to {date_range['latest'].iloc[0]}\")\n",
    "\n",
    "print(\"\\n‚úÖ CRITICAL VALIDATIONS PASSED (warnings noted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cffc5bb",
   "metadata": {},
   "source": [
    "### Gold Layer: Business Metrics\n",
    "\n",
    "Create analysis-ready tables for stakeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4278b45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T10:53:34.782867Z",
     "iopub.status.busy": "2025-10-22T10:53:34.782812Z",
     "iopub.status.idle": "2025-10-22T10:53:34.789098Z",
     "shell.execute_reply": "2025-10-22T10:53:34.788859Z"
    }
   },
   "outputs": [],
   "source": [
    "# GOLD: Business metrics\n",
    "print(\"=== GOLD LAYER (Cafe Sales) ===\\n\")\n",
    "\n",
    "# Metric 1: Product performance\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE gold_product_performance AS\n",
    "    SELECT\n",
    "        item,\n",
    "        COUNT(*) as num_transactions,\n",
    "        SUM(quantity) as total_quantity_sold,\n",
    "        ROUND(AVG(price_per_unit), 2) as avg_price,\n",
    "        ROUND(SUM(total_spent), 2) as total_revenue\n",
    "    FROM silver_cafe_sales\n",
    "    WHERE item IS NOT NULL\n",
    "    GROUP BY item\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Top products by revenue:\")\n",
    "display(con.execute(\"SELECT * FROM gold_product_performance\").df())\n",
    "\n",
    "# Metric 2: Payment method distribution\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE gold_payment_mix AS\n",
    "    SELECT\n",
    "        payment_method,\n",
    "        COUNT(*) as num_transactions,\n",
    "        ROUND(SUM(total_spent), 2) as total_revenue,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 1) as pct_transactions\n",
    "    FROM silver_cafe_sales\n",
    "    WHERE payment_method IS NOT NULL\n",
    "    GROUP BY payment_method\n",
    "    ORDER BY num_transactions DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\\nPayment method distribution:\")\n",
    "display(con.execute(\"SELECT * FROM gold_payment_mix\").df())\n",
    "\n",
    "print(\"\\n‚úÖ Gold layer complete - ready for stakeholder reporting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b660fab9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Key Pipeline Principles\n",
    "\n",
    "### 1. Idempotency\n",
    "> **\"Running the pipeline twice gives the same result.\"**\n",
    "\n",
    "**Good:** Recreate tables from scratch\n",
    "```sql\n",
    "DROP TABLE IF EXISTS gold_daily_sales;\n",
    "CREATE TABLE gold_daily_sales AS \n",
    "SELECT ...\n",
    "```\n",
    "\n",
    "**Why:** Makes pipelines debuggable and reproducible. No \"what state am I in?\" confusion.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fail Fast\n",
    "> **\"If data is bad, stop immediately with a clear error.\"**\n",
    "\n",
    "**Good:**  \n",
    "```python\n",
    "assert df['price'].min() >= 0, \"Negative prices found!\"\n",
    "```\n",
    "\n",
    "**Why:** Bad data = bad decisions. Find problems early, loudly.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Document Assumptions\n",
    "\n",
    "Every validation is documentation:\n",
    "```python\n",
    "assert df['order_id'].is_unique, \"Duplicate order IDs\"\n",
    "assert df['date'].max() <= pd.Timestamp.now(), \"Future dates found\"\n",
    "```\n",
    "\n",
    "**Why:** Future you (and your teammates) need to know what you expected.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c49392",
   "metadata": {},
   "source": [
    "## 7. Work Habits & Best Practices\n",
    "\n",
    "How to work effectively with data and code.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use SQL vs Python\n",
    "\n",
    "**Decision table:**\n",
    "\n",
    "| Task | Use SQL | Use Python |\n",
    "|------|---------|------------|\n",
    "| Filter/subset rows | ‚úÖ `WHERE` clause | Only for complex logic |\n",
    "| Join tables | ‚úÖ Fast, declarative | Complex merge logic |\n",
    "| Aggregate metrics | ‚úÖ `GROUP BY` | Multi-step calculations |\n",
    "| String parsing | Simple (`SPLIT`, `SUBSTRING`) | ‚úÖ Regex, complex rules |\n",
    "| API calls | ‚ùå | ‚úÖ `requests` library |\n",
    "| Date arithmetic | ‚úÖ Built-in functions | Complex timezone logic |\n",
    "| Window functions | ‚úÖ `ROW_NUMBER()`, `LAG()` | When SQL gets unreadable |\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "**1. Revenue by customer segment**\n",
    "```sql\n",
    "-- ‚úÖ SQL: Perfect for this\n",
    "SELECT customer_segment, SUM(revenue) as total_revenue\n",
    "FROM orders\n",
    "GROUP BY customer_segment\n",
    "```\n",
    "\n",
    "**2. Parse nested JSON from API**\n",
    "```python\n",
    "# ‚úÖ Python: SQL's JSON functions are limited\n",
    "import requests\n",
    "response = requests.get('https://api.example.com/data')\n",
    "data = response.json()\n",
    "# Recursive parsing, normalization\n",
    "```\n",
    "\n",
    "**3. Clean messy address strings**\n",
    "```python\n",
    "# ‚úÖ Python: Regex and complex string logic\n",
    "import re\n",
    "df['clean_address'] = df['address'].str.replace(r'\\s+', ' ').str.title()\n",
    "```\n",
    "\n",
    "**Rule of thumb:** Use SQL until it hurts, then switch to Python.\n",
    "\n",
    "---\n",
    "\n",
    "### Debugging Strategies\n",
    "\n",
    "**1. Print-driven debugging**\n",
    "```python\n",
    "# Check intermediate results\n",
    "print(f\"Rows before cleaning: {len(df)}\")\n",
    "df = clean_data(df)\n",
    "print(f\"Rows after cleaning: {len(df)}\")  # Did we lose too many?\n",
    "```\n",
    "\n",
    "**2. Isolate transformations**\n",
    "```python\n",
    "# ‚ùå Hard to debug\n",
    "df = df.pipe(clean).pipe(validate).pipe(transform)\n",
    "\n",
    "# ‚úÖ Easier\n",
    "df = clean(df)\n",
    "print(f\"After clean: {len(df)} rows\")\n",
    "df = validate(df)  # Might assert and fail here ‚Äî good!\n",
    "df = transform(df)\n",
    "```\n",
    "\n",
    "**3. Check types early**\n",
    "```python\n",
    "# After loading data\n",
    "print(df.dtypes)  # Are numbers stored as strings?\n",
    "\n",
    "# In SQL\n",
    "SELECT * FROM information_schema.columns WHERE table_name = 'my_table';\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Documentation Patterns\n",
    "\n",
    "**Data dictionary example:**\n",
    "```markdown\n",
    "## silver_orders\n",
    "\n",
    "| Column | Type | Description | Nulls OK? |\n",
    "|--------|------|-------------|-----------|\n",
    "| order_id | VARCHAR | Unique order identifier | No |\n",
    "| order_date | DATE | When order was placed | Yes (1% missing) |\n",
    "| customer_id | VARCHAR | FK to customers table | No |\n",
    "| status | VARCHAR | delivered, shipped, canceled | No |\n",
    "```\n",
    "\n",
    "**Assumption log:**\n",
    "```python\n",
    "# At top of notebook\n",
    "\"\"\"\n",
    "ASSUMPTIONS:\n",
    "- Order dates before 2015 are data errors (filtered out)\n",
    "- NULL payment_method means Cash (historical system default)\n",
    "- Customer addresses not validated (trust source system)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Git Habits\n",
    "\n",
    "**Small, focused commits:**\n",
    "```bash\n",
    "# ‚úÖ Good\n",
    "git add notebooks/day3_analysis.ipynb\n",
    "git commit -m \"Add bronze layer for cafe sales data\"\n",
    "\n",
    "git add notebooks/day3_analysis.ipynb\n",
    "git commit -m \"Add silver layer with NULL handling\"\n",
    "\n",
    "# ‚ùå Bad\n",
    "git add .\n",
    "git commit -m \"Stuff\"  # What stuff? Why?\n",
    "```\n",
    "\n",
    "**Clear commit messages:**\n",
    "- What changed\n",
    "- Why it changed (if not obvious)\n",
    "\n",
    "**`.gitignore` patterns:**\n",
    "```\n",
    "# Ignore notebook outputs\n",
    "*.ipynb_checkpoints/\n",
    ".ipynb_checkpoints\n",
    "\n",
    "# Ignore data files (if large)\n",
    "data/*.csv\n",
    "!data/sample.csv  # Except small samples\n",
    "\n",
    "# Ignore credentials\n",
    ".env\n",
    "credentials.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Restart & Run All Discipline\n",
    "\n",
    "**Before every commit:**\n",
    "1. Restart kernel\n",
    "2. Run All cells\n",
    "3. Verify no errors\n",
    "\n",
    "**Why:**\n",
    "- Catches hidden state bugs\n",
    "- Ensures reproducibility\n",
    "- Your teammate (or future you) can run it\n",
    "\n",
    "**Common gotcha:**\n",
    "```python\n",
    "# Cell 1\n",
    "df = load_data()\n",
    "\n",
    "# Cell 2\n",
    "df = df[df['price'] > 0]  # Modifies df\n",
    "\n",
    "# Cell 3\n",
    "print(len(df))  # Different every time you re-run cell 2!\n",
    "```\n",
    "\n",
    "**Fix:** Make transformations explicit, don't mutate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ac77c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data in the Wild: Common Issues & Solutions\n",
    "\n",
    "Real-world data is messy. Here's what you'll encounter and how to handle it.\n",
    "\n",
    "---\n",
    "\n",
    "### CSV Traps\n",
    "\n",
    "**Problem:** CSVs have no standard for encoding, separators, or quoting.\n",
    "\n",
    "**Common issues:**\n",
    "```python\n",
    "# Encoding issues\n",
    "# üö® Wrong: Assumes UTF-8\n",
    "df = pd.read_csv('data.csv')  # May fail on accented characters\n",
    "\n",
    "# ‚úÖ Right: Specify or detect encoding\n",
    "df = pd.read_csv('data.csv', encoding='utf-8')  # Or 'latin-1', 'cp1252'\n",
    "\n",
    "# Locale separators (European CSVs often use ; and , for decimals)\n",
    "df = pd.read_csv('data.csv', sep=';', decimal=',')\n",
    "\n",
    "# Header drift: Sometimes data exports include summary rows\n",
    "df = pd.read_csv('data.csv', skiprows=2, skipfooter=1)\n",
    "```\n",
    "\n",
    "**Excel gotchas:**\n",
    "- Automatically converts `2-5` to `Feb 5` (date)\n",
    "- Truncates leading zeros (`00123` ‚Üí `123`)\n",
    "- Scientific notation for long numbers\n",
    "\n",
    "---\n",
    "\n",
    "### Date Handling\n",
    "\n",
    "**Always be explicit** ‚Äî dates are strings until you parse them.\n",
    "\n",
    "```sql\n",
    "-- ‚úÖ Good: Try and validate\n",
    "SELECT \n",
    "    transaction_date,\n",
    "    TRY_CAST(transaction_date AS DATE) as parsed_date,\n",
    "    CASE \n",
    "        WHEN TRY_CAST(transaction_date AS DATE) IS NULL \n",
    "        THEN 'INVALID' \n",
    "        ELSE 'OK' \n",
    "    END as status\n",
    "FROM bronze_table\n",
    "WHERE TRY_CAST(transaction_date AS DATE) IS NULL  -- Find bad dates\n",
    "```\n",
    "\n",
    "**Common patterns:**\n",
    "- Mix of formats: `2023-01-15`, `01/15/2023`, `15-Jan-23`\n",
    "- Partial dates: `2023-01`, `2023-Q1`\n",
    "- Timezone issues: Always store in UTC, display in local\n",
    "- Future dates (typos): Validate `date <= CURRENT_DATE`\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple NULL Representations\n",
    "\n",
    "**The reality:** NULL can be represented many ways.\n",
    "\n",
    "**What you'll see:**\n",
    "- True `NULL` (database null)\n",
    "- Empty string: `\"\"`\n",
    "- Sentinel strings: `\"N/A\"`, `\"UNKNOWN\"`, `\"ERROR\"`, `\"--\"`\n",
    "- Sentinel numbers: `-999`, `-1`, `0`\n",
    "- Whitespace: `\"   \"` (looks empty but isn't)\n",
    "\n",
    "**Your cleaning strategy:**\n",
    "```sql\n",
    "-- Normalize to NULL\n",
    "CASE \n",
    "    WHEN field IN ('', 'N/A', 'UNKNOWN', 'ERROR', '--') THEN NULL\n",
    "    WHEN TRIM(field) = '' THEN NULL  -- Handle whitespace\n",
    "    ELSE field\n",
    "END as cleaned_field\n",
    "```\n",
    "\n",
    "**Remember:**\n",
    "- `NULL` means \"unknown\" or \"not applicable\"\n",
    "- Aggregations **exclude NULLs**: `AVG(price)` ignores NULL prices\n",
    "- Comparisons with NULL: `field = NULL` is **always** `FALSE` ‚Äî use `field IS NULL`\n",
    "\n",
    "---\n",
    "\n",
    "### Categorical Validation\n",
    "\n",
    "**Pattern:** Check values against expected domain.\n",
    "\n",
    "```sql\n",
    "-- Find unexpected categories\n",
    "SELECT DISTINCT payment_method\n",
    "FROM silver_cafe_sales\n",
    "WHERE payment_method NOT IN ('Cash', 'Credit Card', 'Digital Wallet')\n",
    "  AND payment_method IS NOT NULL;\n",
    "```\n",
    "\n",
    "**When to use reference tables:**\n",
    "- Product codes ‚Üí Product names\n",
    "- Country codes (ISO) ‚Üí Country names  \n",
    "- Customer IDs ‚Üí Customer details\n",
    "\n",
    "**Benefit:** Ensures consistency, enables joins\n",
    "\n",
    "---\n",
    "\n",
    "### File Formats: CSV vs Parquet\n",
    "\n",
    "**CSV:**\n",
    "- ‚úÖ Human-readable, universal\n",
    "- ‚ùå No built-in types (everything is string until parsed)\n",
    "- ‚ùå Large file sizes\n",
    "- ‚ùå Slow to read/write\n",
    "\n",
    "**Parquet** (columnar format):\n",
    "- ‚úÖ Built-in types (date, int, float, boolean)\n",
    "- ‚úÖ Compressed (10x smaller than CSV)\n",
    "- ‚úÖ Fast reads (especially for analytics)\n",
    "- ‚ùå Not human-readable (binary format)\n",
    "- ‚úÖ DuckDB can query directly: `SELECT * FROM 'data.parquet'`\n",
    "\n",
    "**When to use Parquet:**\n",
    "- Large datasets (>100MB)\n",
    "- Repeated analysis\n",
    "- Storing intermediate pipeline outputs\n",
    "\n",
    "---\n",
    "\n",
    "### Geodata (Brief Mention)\n",
    "\n",
    "**Common patterns:**\n",
    "- Latitude/Longitude pairs: `(40.7128, -74.0060)` ‚Üí NYC\n",
    "- Addresses ‚Üí Geocoding (convert to lat/lon)\n",
    "- **Caveats:** Geocoding quality varies, address normalization is hard\n",
    "\n",
    "**Beyond this course** ‚Äî but common in business data (store locations, customer addresses, delivery zones)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81905ecb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Pipeline pattern:** Bronze (raw) ‚Üí Silver (clean) ‚Üí Gold (metrics)\n",
    "2. **Two scenarios:** \n",
    "   - E-commerce: Relational integrity (PKs, FKs, joins)\n",
    "   - Retail: Data quality (NULLs, types, calculations)\n",
    "3. **Validations:** Assertions catch problems early and loudly\n",
    "4. **Idempotency:** Re-running gives same result\n",
    "5. **Data in the wild:** Encoding, dates, NULLs, categoricals, file formats\n",
    "6. **Work habits:** SQL vs Python, debugging, docs, git, Restart & Run All\n",
    "\n",
    "**You're not just analyzing data‚Äîyou're building infrastructure.**\n",
    "\n",
    "---\n",
    "\n",
    "## Next: In-Class Exercise\n",
    "\n",
    "Build a mini-pipeline:\n",
    "1. Bronze: Load raw data\n",
    "2. Silver: Clean, validate (2 assertions)\n",
    "3. Gold: Create 2-3 metrics\n",
    "4. Document: Risk note\n",
    "\n",
    "**Time:** 15 minutes\n",
    "**Notebook:** `day3_exercise_mini_pipeline.ipynb`\n",
    "\n",
    "**Let's build!** üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e535657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Course Evaluation\n",
    "\n",
    "**Please complete the course evaluation!**\n",
    "\n",
    "Use the QR code below to start the evaluation. This evaluation is only accessible for registered participants.\n",
    "\n",
    "Your feedback helps improve the course for future students.\n",
    "\n",
    "![Course Evaluation QR Code](../../references/images/course_evaluation_qr.png)\n",
    "\n",
    "**Thank you!** üôè\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
